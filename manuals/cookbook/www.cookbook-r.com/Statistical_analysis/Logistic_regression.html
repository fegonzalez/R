<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML+RDFa 1.0//EN"
    "http://www.w3.org/MarkUp/DTD/xhtml-rdfa-1.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"
      xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
      xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#"
      xmlns:dc="http://purl.org/dc/elements/1.1/"
      xmlns:foaf="http://xmlns.com/foaf/0.1/">
  
  <head>
    
    
      
        <meta http-equiv="Content-Type" content="application/xhtml+xml; charset=UTF-8" />
      
      
      <title>
        Cookbook for R » 
        Logistic regression
      </title>
      
      
        
        <link rel="stylesheet" type="text/css" href="http://fonts.googleapis.com/css?family=Open+Sans:400italic,600italic,400,600" media="screen, projection" />
        <link rel="stylesheet" type="text/css" href="../media/css/mystyle.css" media="screen, projection" />
      
      
      
      
    
  </head>
  
  <body >
    
      
      <div id="header">
        
          <div id="wiki_name">
          
            Cookbook for R
          
          </div>
           
            
  
    <ol id="breadcrumbs">
      
        <li class="crumb-0 not-last">
          
            <a href="../index.html">index</a>
          
        </li>
      
        <li class="crumb-1 not-last">
          
            <a href="../Statistical_analysis.html">Statistical_analysis</a>
          
        </li>
      
        <li class="crumb-2 last">
          
            Logistic_regression
          
        </li>
      
    </ol> <!-- ol#breadcrumbs -->
  

          
        
      </div>
      
      <div id="content">
        
        
        
        <h1 id="logistic-regression">Logistic regression</h1>
<div class="toc"><span class="toctitle">Table of contents</span><ul>
<li><a href="Logistic_regression.html#logistic-regression">Logistic regression</a><ul>
<li><a href="Logistic_regression.html#problem">Problem</a></li>
<li><a href="Logistic_regression.html#solution">Solution</a><ul>
<li><a href="Logistic_regression.html#continuous-predictor-dichotomous-outcome">Continuous predictor, dichotomous outcome</a><ul>
<li><a href="Logistic_regression.html#plotting">Plotting</a></li>
</ul>
</li>
<li><a href="Logistic_regression.html#dichotomous-predictor-dichotomous-outcome">Dichotomous predictor, dichotomous outcome</a><ul>
<li><a href="Logistic_regression.html#plotting_1">Plotting</a></li>
</ul>
</li>
<li><a href="Logistic_regression.html#continuous-and-dichotomous-predictors-dichotomous-outcome">Continuous and dichotomous predictors, dichotomous outcome</a></li>
<li><a href="Logistic_regression.html#multiple-predictors-with-interactions">Multiple predictors with interactions</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<h2 id="problem">Problem</h2>
<p>You want to perform a logistic regression.</p>
<h2 id="solution">Solution</h2>
<p>A logistic regression is typically used when there is one dichotomous outcome variable (such as winning or losing), and a continuous predictor variable which is related to the probability or odds of the outcome variable. It can also be used with categorical predictors, and with multiple predictors.</p>
<p>Suppose we start with part of the built-in <code>mtcars</code> dataset. In the examples below, we_ll use <code>vs</code> as the outcome variable, <code>mpg</code> as a continuous predictor, and <code>am</code> as a categorical (dichotomous) predictor.</p>
<div class="codehilite"><pre>data<span class="p">(</span>mtcars<span class="p">)</span>
df <span class="o">&lt;-</span> <span class="kp">subset</span><span class="p">(</span>mtcars<span class="p">,</span> select<span class="o">=</span><span class="kt">c</span><span class="p">(</span>mpg<span class="p">,</span>am<span class="p">,</span>vs<span class="p">))</span>
<span class="c1">#                      mpg am vs</span>
<span class="c1"># Mazda RX4           21.0  1  0</span>
<span class="c1"># Mazda RX4 Wag       21.0  1  0</span>
<span class="c1"># Datsun 710          22.8  1  1</span>
<span class="c1"># Hornet 4 Drive      21.4  0  1</span>
<span class="c1"># Hornet Sportabout   18.7  0  0</span>
<span class="c1"># Valiant             18.1  0  1</span>
<span class="c1"># Duster 360          14.3  0  0</span>
<span class="c1"># Merc 240D           24.4  0  1</span>
<span class="c1"># Merc 230            22.8  0  1</span>
<span class="c1"># Merc 280            19.2  0  1</span>
<span class="c1"># Merc 280C           17.8  0  1</span>
<span class="c1"># Merc 450SE          16.4  0  0</span>
<span class="c1"># Merc 450SL          17.3  0  0</span>
<span class="c1"># Merc 450SLC         15.2  0  0</span>
<span class="c1"># Cadillac Fleetwood  10.4  0  0</span>
<span class="c1"># Lincoln Continental 10.4  0  0</span>
<span class="c1"># Chrysler Imperial   14.7  0  0</span>
<span class="c1"># Fiat 128            32.4  1  1</span>
<span class="c1"># Honda Civic         30.4  1  1</span>
<span class="c1"># Toyota Corolla      33.9  1  1</span>
<span class="c1"># Toyota Corona       21.5  0  1</span>
<span class="c1"># Dodge Challenger    15.5  0  0</span>
<span class="c1"># AMC Javelin         15.2  0  0</span>
<span class="c1"># Camaro Z28          13.3  0  0</span>
<span class="c1"># Pontiac Firebird    19.2  0  0</span>
<span class="c1"># Fiat X1-9           27.3  1  1</span>
<span class="c1"># Porsche 914-2       26.0  1  0</span>
<span class="c1"># Lotus Europa        30.4  1  1</span>
<span class="c1"># Ford Pantera L      15.8  1  0</span>
<span class="c1"># Ferrari Dino        19.7  1  0</span>
<span class="c1"># Maserati Bora       15.0  1  0</span>
<span class="c1"># Volvo 142E          21.4  1  1</span>
</pre></div>


<h3 id="continuous-predictor-dichotomous-outcome">Continuous predictor, dichotomous outcome</h3>
<p>If the data set has one dichotomous and one continuous variable, and the continuous variable is a predictor of the <strong>probability</strong> the dichotomous variable, then a logistic regression might be appropriate.</p>
<p>In this example, <code>mpg</code> is the continuous predictor variable, and <code>vs</code> is the dichotomous outcome variable.</p>
<div class="codehilite"><pre><span class="c1"># Do the logistic regression - both of these have the same effect.</span>
<span class="c1"># (&quot;logit&quot; is the default model when family is binomial.)</span>
logr.vm <span class="o">&lt;-</span> glm<span class="p">(</span>vs <span class="o">~</span> mpg<span class="p">,</span> data<span class="o">=</span>df<span class="p">,</span> family<span class="o">=</span>binomial<span class="p">)</span>
logr.vm <span class="o">&lt;-</span> glm<span class="p">(</span>vs <span class="o">~</span> mpg<span class="p">,</span> data<span class="o">=</span>df<span class="p">,</span> family<span class="o">=</span>binomial<span class="p">(</span>link<span class="o">=</span><span class="s">&quot;logit&quot;</span><span class="p">))</span>
</pre></div>


<p>To view the model and information about it:</p>
<div class="codehilite"><pre><span class="c1"># Print information about the model</span>
logr.vm
<span class="c1"># Call:  glm(formula = vs ~ mpg, family = binomial(link = &quot;logit&quot;), data = df) </span>
<span class="c1">#</span>
<span class="c1"># Coefficients:</span>
<span class="c1"># (Intercept)          mpg  </span>
<span class="c1">#     -8.8331       0.4304  </span>
<span class="c1">#</span>
<span class="c1"># Degrees of Freedom: 31 Total (i.e. Null);  30 Residual</span>
<span class="c1"># Null Deviance:        43.86 </span>
<span class="c1"># Residual Deviance: 25.53  AIC: 29.53 </span>

<span class="c1"># More information about the model</span>
<span class="kp">summary</span><span class="p">(</span>logr.vm<span class="p">)</span>
<span class="c1"># Call:</span>
<span class="c1"># glm(formula = vs ~ mpg, family = binomial(link = &quot;logit&quot;), data = df)</span>
<span class="c1">#</span>
<span class="c1"># Deviance Residuals: </span>
<span class="c1">#     Min       1Q   Median       3Q      Max  </span>
<span class="c1"># -2.2127  -0.5121  -0.2276   0.6403   1.6980  </span>
<span class="c1">#</span>
<span class="c1"># Coefficients:</span>
<span class="c1">#             Estimate Std. Error z value Pr(&gt;|z|)   </span>
<span class="c1"># (Intercept)  -8.8331     3.1623  -2.793  0.00522 **</span>
<span class="c1"># mpg           0.4304     0.1584   2.717  0.00659 **</span>
<span class="c1"># ---</span>
<span class="c1"># Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 </span>
<span class="c1"># </span>
<span class="c1"># (Dispersion parameter for binomial family taken to be 1)</span>
<span class="c1"># </span>
<span class="c1">#     Null deviance: 43.860  on 31  degrees of freedom</span>
<span class="c1"># Residual deviance: 25.533  on 30  degrees of freedom</span>
<span class="c1"># AIC: 29.533</span>
<span class="c1"># </span>
<span class="c1"># Number of Fisher Scoring iterations: 6</span>
</pre></div>


<h4 id="plotting">Plotting</h4>
<p>The data and logistic regression model can be plotted with ggplot2 or standard graphics:</p>
<div class="codehilite"><pre><span class="kn">library</span><span class="p">(</span>ggplot2<span class="p">)</span>
ggplot<span class="p">(</span>df<span class="p">,</span> aes<span class="p">(</span>x<span class="o">=</span>mpg<span class="p">,</span> y<span class="o">=</span>vs<span class="p">))</span> <span class="o">+</span> geom_point<span class="p">()</span> <span class="o">+</span> 
  stat_smooth<span class="p">(</span>method<span class="o">=</span><span class="s">&quot;glm&quot;</span><span class="p">,</span> family<span class="o">=</span><span class="s">&quot;binomial&quot;</span><span class="p">,</span> se<span class="o">=</span><span class="kc">FALSE</span><span class="p">)</span>


plot<span class="p">(</span>df<span class="o">$</span>mpg<span class="p">,</span> df<span class="o">$</span>vs<span class="p">)</span>
curve<span class="p">(</span>predict<span class="p">(</span>logr.vm<span class="p">,</span> <span class="kt">data.frame</span><span class="p">(</span>mpg<span class="o">=</span>x<span class="p">),</span> type<span class="o">=</span><span class="s">&quot;response&quot;</span><span class="p">),</span> add<span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span> 
</pre></div>


<p><img alt="" src="Logistic_regression/logistic-ggplot2.png" />
<img alt="" src="Logistic_regression/logistic-standard.png" /></p>
<h3 id="dichotomous-predictor-dichotomous-outcome">Dichotomous predictor, dichotomous outcome</h3>
<p>This proceeds in much the same way as above. In this example, <code>am</code> is the dichotomous predictor variable, and <code>vs</code> is the dichotomous outcome variable.</p>
<div class="codehilite"><pre><span class="c1"># Do the logistic regression</span>
logr.va <span class="o">&lt;-</span> glm<span class="p">(</span>vs <span class="o">~</span> am<span class="p">,</span> data<span class="o">=</span>df<span class="p">,</span> family<span class="o">=</span>binomial<span class="p">)</span>

<span class="c1"># Print information about the model</span>
logr.va
<span class="c1"># Call:  glm(formula = vs ~ am, family = binomial(link = &quot;logit&quot;), data = df) </span>
<span class="c1"># </span>
<span class="c1"># Coefficients:</span>
<span class="c1"># (Intercept)           am  </span>
<span class="c1">#     -0.5390       0.6931  </span>
<span class="c1"># </span>
<span class="c1"># Degrees of Freedom: 31 Total (i.e. Null);  30 Residual</span>
<span class="c1"># Null Deviance:        43.86 </span>
<span class="c1"># Residual Deviance: 42.95  AIC: 46.95 </span>

<span class="c1"># More information about the model</span>
<span class="kp">summary</span><span class="p">(</span>logr.va<span class="p">)</span>
<span class="c1"># Call:</span>
<span class="c1"># glm(formula = vs ~ am, family = binomial(link = &quot;logit&quot;), data = df)</span>
<span class="c1"># </span>
<span class="c1"># Deviance Residuals: </span>
<span class="c1">#     Min       1Q   Median       3Q      Max  </span>
<span class="c1"># -1.2435  -0.9587  -0.9587   1.1127   1.4132  </span>
<span class="c1"># </span>
<span class="c1"># Coefficients:</span>
<span class="c1">#             Estimate Std. Error z value Pr(&gt;|z|)</span>
<span class="c1"># (Intercept)  -0.5390     0.4756  -1.133    0.257</span>
<span class="c1"># am            0.6931     0.7319   0.947    0.344</span>
<span class="c1"># </span>
<span class="c1"># (Dispersion parameter for binomial family taken to be 1)</span>
<span class="c1"># </span>
<span class="c1">#     Null deviance: 43.860  on 31  degrees of freedom</span>
<span class="c1"># Residual deviance: 42.953  on 30  degrees of freedom</span>
<span class="c1"># AIC: 46.953</span>
<span class="c1"># </span>
<span class="c1"># Number of Fisher Scoring iterations: 4</span>
</pre></div>


<h4 id="plotting_1">Plotting</h4>
<p>The data and logistic regression model can be plotted with ggplot2 or standard graphics, although the plots are probably less informative than those with a continuous variable. Because there are only 4 locations for the points to go, it will help to jitter the points so they do not all get overplotted.</p>
<div class="codehilite"><pre><span class="kn">library</span><span class="p">(</span>ggplot2<span class="p">)</span>
ggplot<span class="p">(</span>df<span class="p">,</span> aes<span class="p">(</span>x<span class="o">=</span>am<span class="p">,</span> y<span class="o">=</span>vs<span class="p">))</span> <span class="o">+</span> 
  geom_point<span class="p">(</span>shape<span class="o">=</span><span class="m">1</span><span class="p">,</span> position<span class="o">=</span>position_jitter<span class="p">(</span>width<span class="o">=</span><span class="m">.05</span><span class="p">,</span>height<span class="o">=</span><span class="m">.05</span><span class="p">))</span> <span class="o">+</span> 
  stat_smooth<span class="p">(</span>method<span class="o">=</span><span class="s">&quot;glm&quot;</span><span class="p">,</span> family<span class="o">=</span><span class="s">&quot;binomial&quot;</span><span class="p">,</span> se<span class="o">=</span><span class="kc">FALSE</span><span class="p">)</span>


plot<span class="p">(</span><span class="kp">jitter</span><span class="p">(</span>df<span class="o">$</span>am<span class="p">,</span> <span class="m">.2</span><span class="p">),</span> <span class="kp">jitter</span><span class="p">(</span>df<span class="o">$</span>vs<span class="p">,</span> <span class="m">.2</span><span class="p">))</span>
curve<span class="p">(</span>predict<span class="p">(</span>logr.va<span class="p">,</span> <span class="kt">data.frame</span><span class="p">(</span>am<span class="o">=</span>x<span class="p">),</span> type<span class="o">=</span><span class="s">&quot;response&quot;</span><span class="p">),</span> add<span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span> 
</pre></div>


<p><img alt="" src="Logistic_regression/logistic-dichotomous-ggplot2.png" />
<img alt="" src="Logistic_regression/logistic-dichotomous-standard.png" /></p>
<h3 id="continuous-and-dichotomous-predictors-dichotomous-outcome">Continuous and dichotomous predictors, dichotomous outcome</h3>
<p>This is similar to the previous examples. In this example, <code>mpg</code> is the continuous predictor, <code>am</code> is the dichotomous predictor variable, and <code>vs</code> is the dichotomous outcome variable.</p>
<div class="codehilite"><pre>logr.vma <span class="o">&lt;-</span> glm<span class="p">(</span>vs <span class="o">~</span> mpg <span class="o">+</span> am<span class="p">,</span> data<span class="o">=</span>df<span class="p">,</span> family<span class="o">=</span>binomial<span class="p">)</span>

logr.vma
<span class="c1"># Call:  glm(formula = vs ~ mpg + am, family = binomial(link = &quot;logit&quot;),      data = df) </span>
<span class="c1"># </span>
<span class="c1"># Coefficients:</span>
<span class="c1"># (Intercept)          mpg           am  </span>
<span class="c1">#    -12.7051       0.6809      -3.0073  </span>
<span class="c1"># </span>
<span class="c1"># Degrees of Freedom: 31 Total (i.e. Null);  29 Residual</span>
<span class="c1"># Null Deviance:        43.86 </span>
<span class="c1"># Residual Deviance: 20.65  AIC: 26.65 </span>

<span class="kp">summary</span><span class="p">(</span>logr.vma<span class="p">)</span>
<span class="c1"># Call:</span>
<span class="c1"># glm(formula = vs ~ mpg + am, family = binomial(link = &quot;logit&quot;), </span>
<span class="c1">#     data = df)</span>
<span class="c1"># </span>
<span class="c1"># Deviance Residuals: </span>
<span class="c1">#      Min        1Q    Median        3Q       Max  </span>
<span class="c1"># -2.05888  -0.44544  -0.08765   0.33335   1.68405  </span>
<span class="c1"># </span>
<span class="c1"># Coefficients:</span>
<span class="c1">#             Estimate Std. Error z value Pr(&gt;|z|)   </span>
<span class="c1"># (Intercept) -12.7051     4.6252  -2.747  0.00602 **</span>
<span class="c1"># mpg           0.6809     0.2524   2.698  0.00697 **</span>
<span class="c1"># am           -3.0073     1.5995  -1.880  0.06009 . </span>
<span class="c1"># ---</span>
<span class="c1"># Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 </span>
<span class="c1"># </span>
<span class="c1"># (Dispersion parameter for binomial family taken to be 1)</span>
<span class="c1"># </span>
<span class="c1">#    Null deviance: 43.860  on 31  degrees of freedom</span>
<span class="c1"># Residual deviance: 20.646  on 29  degrees of freedom</span>
<span class="c1"># AIC: 26.646</span>
<span class="c1"># </span>
<span class="c1"># Number of Fisher Scoring iterations: 6</span>
</pre></div>


<h3 id="multiple-predictors-with-interactions">Multiple predictors with interactions</h3>
<p>It is possible to test for interactions when there are multiple predictors. The interactions can be specified individually, as with <code>a + b + c + a:b + b:c + a:b:c</code>, or they can be expanded automatically, with <code>a * b * c</code>. It is possible to specify only a subset of the possible interactions, such as <code>a + b + c + a:c</code>.</p>
<p>This case proceeds as above, but with a slight change: instead of the formula being <code>vs ~ mpg + am</code>, it is <code>vs ~ mpg * am</code>, which is equivalent to <code>vs ~ mpg + am + mpg:am</code>.</p>
<div class="codehilite"><pre><span class="c1"># Do the logistic regression - both of these have the same effect.</span>
logr.vmai <span class="o">&lt;-</span> glm<span class="p">(</span>vs <span class="o">~</span> mpg <span class="o">*</span> am<span class="p">,</span> data<span class="o">=</span>df<span class="p">,</span> family<span class="o">=</span>binomial<span class="p">)</span>
logr.vmai <span class="o">&lt;-</span> glm<span class="p">(</span>vs <span class="o">~</span> mpg <span class="o">+</span> am <span class="o">+</span> mpg<span class="o">:</span>am<span class="p">,</span> data<span class="o">=</span>df<span class="p">,</span> family<span class="o">=</span>binomial<span class="p">)</span>

logr.vmai
<span class="c1"># Call:  glm(formula = vs ~ mpg * am, family = binomial, data = df) </span>
<span class="c1">#</span>
<span class="c1"># Coefficients:</span>
<span class="c1"># (Intercept)          mpg           am       mpg:am  </span>
<span class="c1">#    -20.4784       1.1084      10.1055      -0.6637  </span>
<span class="c1"># </span>
<span class="c1"># Degrees of Freedom: 31 Total (i.e. Null);  28 Residual</span>
<span class="c1"># Null Deviance:        43.86 </span>
<span class="c1"># Residual Deviance: 19.12  AIC: 27.12 </span>

<span class="kp">summary</span><span class="p">(</span>logr.vmai<span class="p">)</span>
<span class="c1"># Call:</span>
<span class="c1"># glm(formula = vs ~ mpg + am + mpg:am, family = binomial, data = df)</span>
<span class="c1"># </span>
<span class="c1"># Deviance Residuals: </span>
<span class="c1">#      Min        1Q    Median        3Q       Max  </span>
<span class="c1"># -1.70566  -0.31124  -0.04817   0.28038   1.55603  </span>
<span class="c1"># </span>
<span class="c1"># Coefficients:</span>
<span class="c1">#             Estimate Std. Error z value Pr(&gt;|z|)  </span>
<span class="c1"># (Intercept) -20.4784    10.5525  -1.941   0.0523 .</span>
<span class="c1"># mpg           1.1084     0.5770   1.921   0.0547 .</span>
<span class="c1"># am           10.1055    11.9104   0.848   0.3962  </span>
<span class="c1"># mpg:am       -0.6637     0.6242  -1.063   0.2877  </span>
<span class="c1"># ---</span>
<span class="c1"># Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 </span>
<span class="c1"># </span>
<span class="c1"># (Dispersion parameter for binomial family taken to be 1)</span>
<span class="c1"># </span>
<span class="c1">#     Null deviance: 43.860  on 31  degrees of freedom</span>
<span class="c1"># Residual deviance: 19.125  on 28  degrees of freedom</span>
<span class="c1"># AIC: 27.125</span>
<span class="c1"># </span>
<span class="c1"># Number of Fisher Scoring iterations: 7</span>
</pre></div>


<p>TODO: Add comparison between interaction and non-interaction models.</p>
        
        
        
        
        <hr class="clear" />
      </div> <!-- div#content -->
      
      
      <div id="footer">
        <p>
          
            Cookbook for R —
          
          Powered by <a href="http://markdoc.org/">Markdoc</a>.
          Please report errors to winston@stdout.org.
        </p>
      </div>
      
    
    
    
    <hr class="clear" />

  
    
      <!-- Google Analytics -->
      <script type="text/javascript">
        var _gaq = _gaq || [];
        _gaq.push(['_setAccount', 'UA-28886507-2']);
        _gaq.push(['_trackPageview']);
        (function() {
          var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
          ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
          (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(ga);
        })();
      </script>
    
  

  </body>
</html>