<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML+RDFa 1.0//EN"
    "http://www.w3.org/MarkUp/DTD/xhtml-rdfa-1.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"
      xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
      xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#"
      xmlns:dc="http://purl.org/dc/elements/1.1/"
      xmlns:foaf="http://xmlns.com/foaf/0.1/">
  
  <head>
    
    
      
        <meta http-equiv="Content-Type" content="application/xhtml+xml; charset=UTF-8" />
      
      
      <title>
        Cookbook for R Â» 
        Inter-rater reliability
      </title>
      
      
        
        <link rel="stylesheet" type="text/css" href="http://fonts.googleapis.com/css?family=Open+Sans:400italic,600italic,400,600" media="screen, projection" />
        <link rel="stylesheet" type="text/css" href="../media/css/mystyle.css" media="screen, projection" />
      
      
      
      
    
  </head>
  
  <body >
    
      
      <div id="header">
        
          <div id="wiki_name">
          
            Cookbook for R
          
          </div>
           
            
  
    <ol id="breadcrumbs">
      
        <li class="crumb-0 not-last">
          
            <a href="../index.html">index</a>
          
        </li>
      
        <li class="crumb-1 not-last">
          
            <a href="../Statistical_analysis.html">Statistical_analysis</a>
          
        </li>
      
        <li class="crumb-2 last">
          
            Inter-rater_reliability
          
        </li>
      
    </ol> <!-- ol#breadcrumbs -->
  

          
        
      </div>
      
      <div id="content">
        
        
        
        <h1 id="inter-rater-reliability">Inter-rater reliability</h1>
<div class="toc"><span class="toctitle">Table of contents</span><ul>
<li><a href="Inter-rater_reliability.html#inter-rater-reliability">Inter-rater reliability</a><ul>
<li><a href="Inter-rater_reliability.html#problem">Problem</a></li>
<li><a href="Inter-rater_reliability.html#solution">Solution</a><ul>
<li><a href="Inter-rater_reliability.html#categorical-data">Categorical data</a><ul>
<li><a href="Inter-rater_reliability.html#two-raters-cohens-kappa">Two raters: Cohen's Kappa</a></li>
<li><a href="Inter-rater_reliability.html#n-raters-fleisss-kappa-congers-kappa">N raters: Fleiss's Kappa, Conger's Kappa</a></li>
</ul>
</li>
<li><a href="Inter-rater_reliability.html#ordinal-data-weighted-kappa">Ordinal data: weighted Kappa</a><ul>
<li><a href="Inter-rater_reliability.html#weighted-kappa-with-factors">Weighted Kappa with factors</a></li>
</ul>
</li>
<li><a href="Inter-rater_reliability.html#continuous-data-intraclass-correlation-coefficient">Continuous data: Intraclass correlation coefficient</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<h2 id="problem">Problem</h2>
<p>You want to calculate inter-rater reliability.</p>
<h2 id="solution">Solution</h2>
<p>The method for calculating inter-rater reliability will depend on the type of data (categorical, ordinal, or continuous) and the number of coders.</p>
<h3 id="categorical-data">Categorical data</h3>
<p>Suppose this is your data set. It consists of 30 cases, rated by three coders. It is a subset of the <code>diagnoses</code> data set in the irr package.</p>
<div class="codehilite"><pre><span class="kn">library</span><span class="p">(</span>irr<span class="p">)</span>

data<span class="p">(</span>diagnoses<span class="p">)</span>
df <span class="o">&lt;-</span> diagnoses<span class="p">[,</span><span class="m">1</span><span class="o">:</span><span class="m">3</span><span class="p">]</span>
<span class="c1">#                  rater1                  rater2                  rater3</span>
<span class="c1">#             4. Neurosis             4. Neurosis             4. Neurosis</span>
<span class="c1"># 2. Personality Disorder 2. Personality Disorder 2. Personality Disorder</span>
<span class="c1"># 2. Personality Disorder        3. Schizophrenia        3. Schizophrenia</span>
<span class="c1">#                5. Other                5. Other                5. Other</span>
<span class="c1"># 2. Personality Disorder 2. Personality Disorder 2. Personality Disorder</span>
<span class="c1">#           1. Depression           1. Depression        3. Schizophrenia</span>
<span class="c1">#        3. Schizophrenia        3. Schizophrenia        3. Schizophrenia</span>
<span class="c1">#           1. Depression           1. Depression        3. Schizophrenia</span>
<span class="c1">#           1. Depression           1. Depression             4. Neurosis</span>
<span class="c1">#                5. Other                5. Other                5. Other</span>
<span class="c1">#           1. Depression             4. Neurosis             4. Neurosis</span>
<span class="c1">#           1. Depression 2. Personality Disorder             4. Neurosis</span>
<span class="c1"># 2. Personality Disorder 2. Personality Disorder 2. Personality Disorder</span>
<span class="c1">#           1. Depression             4. Neurosis             4. Neurosis</span>
<span class="c1"># 2. Personality Disorder 2. Personality Disorder             4. Neurosis</span>
<span class="c1">#        3. Schizophrenia        3. Schizophrenia        3. Schizophrenia</span>
<span class="c1">#           1. Depression           1. Depression           1. Depression</span>
<span class="c1">#           1. Depression           1. Depression           1. Depression</span>
<span class="c1"># 2. Personality Disorder 2. Personality Disorder             4. Neurosis</span>
<span class="c1">#           1. Depression        3. Schizophrenia        3. Schizophrenia</span>
<span class="c1">#                5. Other                5. Other                5. Other</span>
<span class="c1"># 2. Personality Disorder             4. Neurosis             4. Neurosis</span>
<span class="c1"># 2. Personality Disorder 2. Personality Disorder             4. Neurosis</span>
<span class="c1">#           1. Depression           1. Depression             4. Neurosis</span>
<span class="c1">#           1. Depression             4. Neurosis             4. Neurosis</span>
<span class="c1"># 2. Personality Disorder 2. Personality Disorder 2. Personality Disorder</span>
<span class="c1">#           1. Depression           1. Depression           1. Depression</span>
<span class="c1"># 2. Personality Disorder 2. Personality Disorder             4. Neurosis</span>
<span class="c1">#           1. Depression        3. Schizophrenia        3. Schizophrenia</span>
<span class="c1">#                5. Other                5. Other                5. Other</span>
</pre></div>


<h4 id="two-raters-cohens-kappa">Two raters: Cohen's Kappa</h4>
<p>This will calculate Cohen's Kappa for two coders -- In this case, raters 1 and 2.</p>
<div class="codehilite"><pre>kappa2<span class="p">(</span>df<span class="p">[,</span><span class="kt">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">2</span><span class="p">)],</span> <span class="s">&quot;unweighted&quot;</span><span class="p">)</span>
<span class="c1"># Cohen&#39;s Kappa for 2 Raters (Weights: unweighted)</span>
<span class="c1">#</span>
<span class="c1"># Subjects = 30 </span>
<span class="c1">#   Raters = 2 </span>
<span class="c1">#    Kappa = 0.651 </span>
<span class="c1">#</span>
<span class="c1">#        z = 7 </span>
<span class="c1">#  p-value = 2.63e-12 </span>
</pre></div>


<h4 id="n-raters-fleisss-kappa-congers-kappa">N raters: Fleiss's Kappa, Conger's Kappa</h4>
<p>If there are more than two raters, use Fleiss's Kappa.</p>
<div class="codehilite"><pre>kappam.fleiss<span class="p">(</span>df<span class="p">)</span>
<span class="c1"># Fleiss&#39; Kappa for m Raters</span>
<span class="c1">#</span>
<span class="c1"># Subjects = 30 </span>
<span class="c1">#   Raters = 3 </span>
<span class="c1">#    Kappa = 0.534 </span>
<span class="c1">#</span>
<span class="c1">#        z = 9.89 </span>
<span class="c1">#  p-value = 0 </span>
</pre></div>


<p>It is also possible to use Conger's (1980) exact Kappa. (Note that it is not clear to me when it is better or worse to use the exact method.)</p>
<div class="codehilite"><pre>kappam.fleiss<span class="p">(</span>df<span class="p">,</span> exact<span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span>
<span class="c1"># Fleiss&#39; Kappa for m Raters (exact value)</span>
<span class="c1">#</span>
<span class="c1"># Subjects = 30 </span>
<span class="c1">#   Raters = 3 </span>
<span class="c1">#    Kappa = 0.55 </span>
</pre></div>


<h3 id="ordinal-data-weighted-kappa">Ordinal data: weighted Kappa</h3>
<p>If the data is ordinal, then it may be appropriate to use a <strong>weighted</strong> Kappa. For example, if the possible values are low, medium, and high, then if a case were rated medium and high by the two coders, they would be in better agreement than if the ratings were low and high.</p>
<p>We will use a subset of the <code>anxiety</code> data set from the irr package.</p>
<div class="codehilite"><pre><span class="kn">library</span><span class="p">(</span>irr<span class="p">)</span>
data<span class="p">(</span>anxiety<span class="p">)</span>

dfa <span class="o">&lt;-</span> anxiety<span class="p">[,</span><span class="kt">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">2</span><span class="p">)]</span>
<span class="c1"># rater1 rater2</span>
<span class="c1">#      3      3</span>
<span class="c1">#      3      6</span>
<span class="c1">#      3      4</span>
<span class="c1">#      4      6</span>
<span class="c1">#      5      2</span>
<span class="c1">#      5      4</span>
<span class="c1">#      2      2</span>
<span class="c1">#      3      4</span>
<span class="c1">#      5      3</span>
<span class="c1">#      2      3</span>
<span class="c1">#      2      2</span>
<span class="c1">#      6      3</span>
<span class="c1">#      1      3</span>
<span class="c1">#      5      3</span>
<span class="c1">#      2      2</span>
<span class="c1">#      2      2</span>
<span class="c1">#      1      1</span>
<span class="c1">#      2      3</span>
<span class="c1">#      4      3</span>
<span class="c1">#      3      4</span>
</pre></div>


<p>The weighted Kappa calculation must be made with 2 raters, and can use either <strong>linear</strong> or <strong>squared</strong> weights of the differences.</p>
<div class="codehilite"><pre><span class="c1"># Compare raters 1 and 2 with squared weights</span>
kappa2<span class="p">(</span>dfa<span class="p">,</span> <span class="s">&quot;squared&quot;</span><span class="p">)</span>
<span class="c1"># Cohen&#39;s Kappa for 2 Raters (Weights: squared)</span>
<span class="c1">#</span>
<span class="c1"># Subjects = 20 </span>
<span class="c1">#   Raters = 2 </span>
<span class="c1">#    Kappa = 0.297 </span>
<span class="c1">#</span>
<span class="c1">#        z = 1.34 </span>
<span class="c1">#  p-value = 0.18 </span>


<span class="c1"># Use linear weights</span>
kappa2<span class="p">(</span>dfa<span class="p">,</span> <span class="s">&quot;equal&quot;</span><span class="p">)</span>
<span class="c1"># Cohen&#39;s Kappa for 2 Raters (Weights: equal)</span>
<span class="c1">#</span>
<span class="c1"># Subjects = 20 </span>
<span class="c1">#   Raters = 2 </span>
<span class="c1">#    Kappa = 0.189 </span>
<span class="c1">#</span>
<span class="c1">#        z = 1.42 </span>
<span class="c1">#  p-value = 0.157 </span>
</pre></div>


<p>Compare the results above to the <strong>unweighted</strong> calculation (used in the tests for non-ordinal data above), which treats all differences as the same:</p>
<div class="codehilite"><pre>kappa2<span class="p">(</span>dfa<span class="p">,</span> <span class="s">&quot;unweighted&quot;</span><span class="p">)</span>
<span class="c1"># Cohen&#39;s Kappa for 2 Raters (Weights: unweighted)</span>
<span class="c1">#</span>
<span class="c1"># Subjects = 20 </span>
<span class="c1">#   Raters = 2 </span>
<span class="c1">#    Kappa = 0.119 </span>
<span class="c1">#</span>
<span class="c1">#        z = 1.16 </span>
<span class="c1">#  p-value = 0.245 </span>
</pre></div>


<h4 id="weighted-kappa-with-factors">Weighted Kappa with factors</h4>
<p>The data above is numeric, but a weighted Kappa can also be calculated for factors. Note that the factor levels must be in the correct order, or results will be wrong.</p>
<div class="codehilite"><pre><span class="c1"># Make a factor-ized version of the data</span>
dfa2 <span class="o">&lt;-</span> dfa
dfa2<span class="o">$</span>rater1 <span class="o">&lt;-</span> <span class="kp">factor</span><span class="p">(</span>dfa2<span class="o">$</span>rater1<span class="p">,</span> levels<span class="o">=</span><span class="m">1</span><span class="o">:</span><span class="m">6</span><span class="p">,</span> labels<span class="o">=</span><span class="kc">LETTERS</span><span class="p">[</span><span class="m">1</span><span class="o">:</span><span class="m">6</span><span class="p">])</span>
dfa2<span class="o">$</span>rater2 <span class="o">&lt;-</span> <span class="kp">factor</span><span class="p">(</span>dfa2<span class="o">$</span>rater2<span class="p">,</span> levels<span class="o">=</span><span class="m">1</span><span class="o">:</span><span class="m">6</span><span class="p">,</span> labels<span class="o">=</span><span class="kc">LETTERS</span><span class="p">[</span><span class="m">1</span><span class="o">:</span><span class="m">6</span><span class="p">])</span>
<span class="c1"># rater1 rater2</span>
<span class="c1">#      C      C</span>
<span class="c1">#      C      F</span>
<span class="c1">#      C      D</span>
<span class="c1">#      D      F</span>
<span class="c1">#      E      B</span>
<span class="c1">#      E      D</span>
<span class="c1">#      B      B</span>
<span class="c1">#      C      D</span>
<span class="c1">#      E      C</span>
<span class="c1">#      B      C</span>
<span class="c1">#      B      B</span>
<span class="c1">#      F      C</span>
<span class="c1">#      A      C</span>
<span class="c1">#      E      C</span>
<span class="c1">#      B      B</span>
<span class="c1">#      B      B</span>
<span class="c1">#      A      A</span>
<span class="c1">#      B      C</span>
<span class="c1">#      D      C</span>
<span class="c1">#      C      D</span>

<span class="c1"># The factor levels must be in the correct order:</span>
<span class="kp">levels</span><span class="p">(</span>dfa2<span class="o">$</span>rater1<span class="p">)</span>
<span class="c1"># &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; &quot;F&quot;</span>
<span class="kp">levels</span><span class="p">(</span>dfa2<span class="o">$</span>rater2<span class="p">)</span>
<span class="c1"># &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; &quot;F&quot;</span>


<span class="c1"># The results are the same as with the numeric data, above</span>
kappa2<span class="p">(</span>dfa2<span class="p">,</span> <span class="s">&quot;squared&quot;</span><span class="p">)</span>
<span class="c1"># Cohen&#39;s Kappa for 2 Raters (Weights: squared)</span>
<span class="c1">#</span>
<span class="c1"># Subjects = 20 </span>
<span class="c1">#   Raters = 2 </span>
<span class="c1">#    Kappa = 0.297 </span>
<span class="c1">#</span>
<span class="c1">#        z = 1.34 </span>
<span class="c1">#  p-value = 0.18 </span>


<span class="c1"># Use linear weights</span>
kappa2<span class="p">(</span>dfa2<span class="p">,</span> <span class="s">&quot;equal&quot;</span><span class="p">)</span>
<span class="c1"># Cohen&#39;s Kappa for 2 Raters (Weights: equal)</span>
<span class="c1">#</span>
<span class="c1"># Subjects = 20 </span>
<span class="c1">#   Raters = 2 </span>
<span class="c1">#    Kappa = 0.189 </span>
<span class="c1">#</span>
<span class="c1">#        z = 1.42 </span>
<span class="c1">#  p-value = 0.157 </span>
</pre></div>


<h3 id="continuous-data-intraclass-correlation-coefficient">Continuous data: Intraclass correlation coefficient</h3>
<p>When the variable is continuous, the intraclass correlation coefficient should be computed. From the documentation for <code>icc</code>:</p>
<p>When considering which form of ICC is appropriate for an actual set of data, one has take several decisions (Shrout &amp; Fleiss, 1979):
 1. Should only the subjects be considered as random effects (<code>"oneway"</code> model, default) or are subjects and raters randomly chosen from a bigger pool of persons (<code>"twoway"</code> model).
 1. If differences in judges' mean ratings are of interest, interrater <code>"agreement"</code> instead of <code>"consistency"</code> (default) should be computed.
 1. If the unit of analysis is a mean of several ratings, <code>unit</code> should be changed to <code>"average"</code>. In most cases, however, single values (<code>unit="single"</code>, default) are regarded.</p>
<p>We will use the <code>anxiety</code> data set from the irr package.</p>
<div class="codehilite"><pre><span class="kn">library</span><span class="p">(</span>irr<span class="p">)</span>
data<span class="p">(</span>anxiety<span class="p">)</span>
<span class="c1"># rater1 rater2 rater3</span>
<span class="c1">#      3      3      2</span>
<span class="c1">#      3      6      1</span>
<span class="c1">#      3      4      4</span>
<span class="c1">#      4      6      4</span>
<span class="c1">#      5      2      3</span>
<span class="c1">#      5      4      2</span>
<span class="c1">#      2      2      1</span>
<span class="c1">#      3      4      6</span>
<span class="c1">#      5      3      1</span>
<span class="c1">#      2      3      1</span>
<span class="c1">#      2      2      1</span>
<span class="c1">#      6      3      2</span>
<span class="c1">#      1      3      3</span>
<span class="c1">#      5      3      3</span>
<span class="c1">#      2      2      1</span>
<span class="c1">#      2      2      1</span>
<span class="c1">#      1      1      3</span>
<span class="c1">#      2      3      3</span>
<span class="c1">#      4      3      2</span>
<span class="c1">#      3      4      2</span>


<span class="c1"># Just one of the many possible ICC coefficients</span>
icc<span class="p">(</span>anxiety<span class="p">,</span> model<span class="o">=</span><span class="s">&quot;twoway&quot;</span><span class="p">,</span> type<span class="o">=</span><span class="s">&quot;agreement&quot;</span><span class="p">)</span>
<span class="c1"># Single Score Intraclass Correlation</span>
<span class="c1">#</span>
<span class="c1">#   Model: twoway </span>
<span class="c1">#   Type : agreement </span>
<span class="c1">#</span>
<span class="c1">#   Subjects = 20 </span>
<span class="c1">#     Raters = 3 </span>
<span class="c1">#   ICC(A,1) = 0.198</span>
<span class="c1">#</span>
<span class="c1"># F-Test, H0: r0 = 0 ; H1: r0 &gt; 0 </span>
<span class="c1"># F(19,39.7) = 1.83 , p = 0.0543 </span>
<span class="c1">#</span>
<span class="c1"># 95%-Confidence Interval for ICC Population Values:</span>
<span class="c1">#  -0.039 &lt; ICC &lt; 0.494</span>
</pre></div>
        
        
        
        
        <hr class="clear" />
      </div> <!-- div#content -->
      
      
      <div id="footer">
        <p>
          
            Cookbook for R â
          
          Powered by <a href="http://markdoc.org/">Markdoc</a>.
          Please report errors to winston@stdout.org.
        </p>
      </div>
      
    
    
    
    <hr class="clear" />

  
    
      <!-- Google Analytics -->
      <script type="text/javascript">
        var _gaq = _gaq || [];
        _gaq.push(['_setAccount', 'UA-28886507-2']);
        _gaq.push(['_trackPageview']);
        (function() {
          var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
          ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
          (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(ga);
        })();
      </script>
    
  

  </body>
</html>